{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMi2tQoOltG/mEVFipoaT5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tthandi/magic_formula_project/blob/main/MarketCap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uPnt98IYTER"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "alpha_vantage_market_cap_full.py\n",
        "\n",
        "â€¢ Computes market-cap snapshots for ALL tickers and ALL balance-sheet dates\n",
        "  where Alpha Vantage reports commonStockSharesOutstanding (annual + quarterly).\n",
        "â€¢ Matches each fiscal date to the most recent trading day â‰¤ that date using\n",
        "  TIME_SERIES_DAILY_ADJUSTED (adjusted close).\n",
        "â€¢ Robust:\n",
        "  - API key via env var ALPHAVANTAGE_API_KEY (required)\n",
        "  - Async + rate-limited (CALLS_PER_MIN=75 for premium)\n",
        "  - Success-only checkpoint + failure log (Colab Drive if available)\n",
        "  - Safe CSV loads (mkstemp, fixed column order), partitioned by yyyymm\n",
        "  - Per-partition flush locks (avoid racey double-flush)\n",
        "  - End-of-run de-dup per partition on (symbol, snapshotDate)\n",
        "\n",
        "Modes are implicit: this script processes the AV universe minus checkpointed\n",
        "symbols; re-runs will resume automatically.\n",
        "\n",
        "Env:\n",
        "  ALPHAVANTAGE_API_KEY (required)\n",
        "  GOOGLE_APPLICATION_CREDENTIALS (optional, service account JSON for BigQuery)\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROJECT   = \"\"\n",
        "DATASET   = \"\"\n",
        "TABLE     = \"market_cap\"\n",
        "\n",
        "import os as _os\n",
        "API_KEY   = '' # REQUIRED\n",
        "\n",
        "CALLS_PER_MIN = 75       # premium\n",
        "CONCURRENCY   = 32       # parallel symbols (two AV calls each)\n",
        "FLUSH_ROWS    = 5_000\n",
        "AUTO_SAVE_SEC = 300\n",
        "\n",
        "CK_DIR_NAME   = \"alpha_vantage_ckpt\"\n",
        "CK_FILE_NAME  = \"av_done_market_cap2.txt\"\n",
        "FAIL_FILE     = \"av_failed_market_cap2.txt\"\n",
        "\n",
        "PART_START, PART_END = 198001, 204001  # yyyymm\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os as _os\n",
        "import io, csv, json, time, asyncio, tempfile, aiohttp, aiolimiter, nest_asyncio, argparse, datetime as dt\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tqdm\n",
        "from aiolimiter import AsyncLimiter\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions as gex\n",
        "from google.auth.exceptions import DefaultCredentialsError, RefreshError\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) BigQuery client (auth-robust) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_bq_client() -> bigquery.Client:\n",
        "    # Colab user auth\n",
        "    try:\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "        colab_auth.authenticate_user()\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Service account\n",
        "    sa_path = _os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    if sa_path and _os.path.exists(sa_path):\n",
        "        creds = service_account.Credentials.from_service_account_file(sa_path)\n",
        "        return bigquery.Client(project=PROJECT, credentials=creds)\n",
        "    # ADC (gcloud)\n",
        "    try:\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except (DefaultCredentialsError, RefreshError) as e:\n",
        "        raise RuntimeError(\n",
        "            \"No Google Cloud credentials found. Use Colab auth, a service account via \"\n",
        "            \"GOOGLE_APPLICATION_CREDENTIALS, or `gcloud auth application-default login`.\"\n",
        "        ) from e\n",
        "\n",
        "bq = get_bq_client()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) Colab Drive checkpoint (if available) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def colab_drive_paths():\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        root = \"/content/drive/MyDrive\"\n",
        "        _os.makedirs(_os.path.join(root, CK_DIR_NAME), exist_ok=True)\n",
        "        ck = _os.path.join(root, CK_DIR_NAME, CK_FILE_NAME)\n",
        "        ff = _os.path.join(root, CK_DIR_NAME, FAIL_FILE)\n",
        "        return ck, ff\n",
        "    except Exception:\n",
        "        return CK_FILE_NAME, FAIL_FILE\n",
        "\n",
        "CHECKFILE, FAILFILE = colab_drive_paths()\n",
        "_tqdmw = tqdm.tqdm.write\n",
        "_tqdmw(f\"âœ… Checkpoint â†’ {CHECKFILE}\")\n",
        "\n",
        "_ck_handle = open(CHECKFILE, \"a\", buffering=1)\n",
        "_ck_lock   = asyncio.Lock()\n",
        "\n",
        "async def mark_done(sym: str):\n",
        "    async with _ck_lock:\n",
        "        _ck_handle.write(sym + \"\\n\"); _ck_handle.flush()\n",
        "        _os.fsync(_ck_handle.fileno())\n",
        "\n",
        "def read_checkpoint() -> Set[str]:\n",
        "    try:    return set(open(CHECKFILE).read().splitlines())\n",
        "    except FileNotFoundError: return set()\n",
        "\n",
        "def mark_failure(sym: str):\n",
        "    with open(FAILFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) Alpha Vantage helpers (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_URL     = \"https://www.alphavantage.co/query\"\n",
        "RATE_LIMITER = AsyncLimiter(CALLS_PER_MIN, 60)\n",
        "\n",
        "def date_to_yyyymm(d: dt.date) -> int: return d.year * 100 + d.month\n",
        "\n",
        "async def av_get(session: aiohttp.ClientSession, **params):\n",
        "    \"\"\"Generic AV GET; returns JSON dict or raw text depending on response.\"\"\"\n",
        "    params |= {\"apikey\": API_KEY}\n",
        "    async with RATE_LIMITER:\n",
        "        try:\n",
        "            async with session.get(BASE_URL, params=params, timeout=45) as r:\n",
        "                if r.status != 200: return None\n",
        "                ct = r.headers.get(\"Content-Type\",\"\")\n",
        "                if \"json\" in ct:\n",
        "                    return await r.json(content_type=None)\n",
        "                return await r.text()\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "            return None\n",
        "\n",
        "async def listing(session: aiohttp.ClientSession, state: str, max_retries: int = 6) -> List[str]:\n",
        "    \"\"\"Robust LISTING_STATUS (CSV) with backoff; returns list of stock symbols.\"\"\"\n",
        "    params = {\"function\":\"LISTING_STATUS\",\"state\":state,\"datatype\":\"csv\",\"apikey\":API_KEY}\n",
        "    backoff = 2\n",
        "    for attempt in range(1, max_retries+1):\n",
        "        async with RATE_LIMITER:\n",
        "            try:\n",
        "                async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                    status = resp.status\n",
        "                    txt = await resp.text()\n",
        "            except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "                _tqdmw(f\"âš ï¸ LISTING_STATUS {state}: network error (try {attempt})\")\n",
        "                await asyncio.sleep(backoff); backoff *= 2; continue\n",
        "        if txt.lstrip().startswith(\"{\"):\n",
        "            try: j = json.loads(txt)\n",
        "            except json.JSONDecodeError: j = {}\n",
        "            msg = j.get(\"Note\") or j.get(\"Information\") or j.get(\"Error Message\") or f\"HTTP {status}\"\n",
        "            _tqdmw(f\"âš ï¸ LISTING_STATUS {state}: {msg} (try {attempt})\")\n",
        "            await asyncio.sleep(backoff); backoff = min(backoff*2, 30); continue\n",
        "        rdr = csv.DictReader(io.StringIO(txt))\n",
        "        syms = [r[\"symbol\"] for r in rdr if r.get(\"assetType\")==\"Stock\" and r.get(\"symbol\")]\n",
        "        if syms: return syms\n",
        "        _tqdmw(f\"âš ï¸ LISTING_STATUS {state}: empty CSV (try {attempt})\")\n",
        "        await asyncio.sleep(backoff); backoff *= 2\n",
        "    _tqdmw(f\"â— LISTING_STATUS {state}: giving up after {max_retries} attempts\")\n",
        "    return []\n",
        "\n",
        "async def fetch_balance_sheets(session: aiohttp.ClientSession, sym: str) -> pd.DataFrame | None:\n",
        "    \"\"\"Return DataFrame of ALL (annual + quarterly) balance-sheet snapshots with shares.\"\"\"\n",
        "    data = await av_get(session, function=\"BALANCE_SHEET\", symbol=sym)\n",
        "    if not data: return None\n",
        "    reports = (data.get(\"annualReports\", []) + data.get(\"quarterlyReports\", []))\n",
        "    if not reports: return None\n",
        "    df = pd.DataFrame(reports)\n",
        "    if \"commonStockSharesOutstanding\" not in df.columns: return None\n",
        "    df = df.replace({\"None\": pd.NA, \"null\": pd.NA, \"\": pd.NA})\n",
        "    df = df[df[\"commonStockSharesOutstanding\"].notna()]\n",
        "    if df.empty: return None\n",
        "\n",
        "    df[\"symbol\"]           = sym\n",
        "    df[\"fiscalDateEnding\"] = pd.to_datetime(df[\"fiscalDateEnding\"]).dt.date\n",
        "    df[\"reportedCurrency\"] = df.get(\"reportedCurrency\", pd.NA)\n",
        "    df[\"commonStockSharesOutstanding\"] = pd.to_numeric(df[\"commonStockSharesOutstanding\"], errors=\"coerce\")\n",
        "    df[\"yyyymm\"]           = df[\"fiscalDateEnding\"].map(date_to_yyyymm)\n",
        "\n",
        "    return df[[\"symbol\",\"fiscalDateEnding\",\"reportedCurrency\",\"commonStockSharesOutstanding\",\"yyyymm\"]]\n",
        "\n",
        "async def fetch_price_series(session: aiohttp.ClientSession, sym: str, retries=1) -> pd.Series | None:\n",
        "    \"\"\"\n",
        "    Return a Series of adjusted_close indexed by DATE (normalize to midnight).\n",
        "    Retries once on JSON throttle.\n",
        "    \"\"\"\n",
        "    txt = await av_get(session,\n",
        "                       function=\"TIME_SERIES_DAILY_ADJUSTED\",\n",
        "                       symbol=sym,\n",
        "                       outputsize=\"full\",\n",
        "                       datatype=\"csv\")\n",
        "\n",
        "    # Throttle/error returns JSON dict\n",
        "    if isinstance(txt, dict):\n",
        "        if retries:\n",
        "            await asyncio.sleep(60)\n",
        "            return await fetch_price_series(session, sym, retries-1)\n",
        "        return None\n",
        "\n",
        "    if not txt or txt.lstrip().startswith(\"{\"):\n",
        "        return None\n",
        "\n",
        "    df = pd.read_csv(io.StringIO(txt), parse_dates=[\"timestamp\"])\n",
        "    # Normalize index to date (remove time) so lookups by dt.date work reliably\n",
        "    df[\"date\"] = df[\"timestamp\"].dt.date\n",
        "    df = df.set_index(\"date\")\n",
        "    return df[\"adjusted_close\"]\n",
        "\n",
        "def price_on_or_before(series: pd.Series, target_date: dt.date) -> tuple[dt.date, float] | None:\n",
        "    \"\"\"Get last available (date, price) where date <= target_date.\"\"\"\n",
        "    if series is None or series.empty: return None\n",
        "    # Use index slice up to target_date, then take the last row if any.\n",
        "    try:\n",
        "        s = series.loc[:target_date]\n",
        "        if s.size == 0: return None\n",
        "        d = s.index[-1]\n",
        "        return d, float(s.iloc[-1])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6) BigQuery init (partitioned table) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try: bq.get_dataset(f\"{PROJECT}.{DATASET}\")\n",
        "except gex.NotFound: bq.create_dataset(bigquery.Dataset(f\"{PROJECT}.{DATASET}\"))\n",
        "\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"symbol\",\"STRING\"),\n",
        "    bigquery.SchemaField(\"snapshotDate\",\"DATE\"),\n",
        "    bigquery.SchemaField(\"reportedCurrency\",\"STRING\"),\n",
        "    bigquery.SchemaField(\"commonStockSharesOutstanding\",\"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"adjustedClose\",\"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"marketCap\",\"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"yyyymm\",\"INT64\"),\n",
        "]\n",
        "table_ref = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "if not any(t.table_id == TABLE for t in bq.list_tables(DATASET)):\n",
        "    tbl = bigquery.Table(table_ref, schema=schema)\n",
        "    tbl.range_partitioning = bigquery.RangePartitioning(\n",
        "        field=\"yyyymm\", range_=bigquery.PartitionRange(PART_START, PART_END, 1))\n",
        "    tbl.clustering_fields = [\"symbol\"]\n",
        "    bq.create_table(tbl)\n",
        "    _tqdmw(f\"âž¡ï¸  Created table {table_ref}\")\n",
        "\n",
        "CSV_COLS = [f.name for f in bq.get_table(table_ref).schema]\n",
        "\n",
        "def load_to_bq(df: pd.DataFrame, yyyymm: int, retries: int = 4):\n",
        "    \"\"\"Append rows to the specific partition (table$yyyymm).\"\"\"\n",
        "    if df is None or df.empty: return\n",
        "    df = df.reindex(columns=CSV_COLS)\n",
        "    fd, tmp = tempfile.mkstemp(suffix=\".csv.gz\"); _os.close(fd)\n",
        "    df.to_csv(tmp, index=False, compression=\"gzip\", date_format=\"%Y-%m-%d\")\n",
        "\n",
        "    dest = f\"{table_ref}${yyyymm}\"\n",
        "    cfg = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "    )\n",
        "    backoff = 5\n",
        "    for attempt in range(1, retries+1):\n",
        "        try:\n",
        "            with open(tmp, \"rb\") as fh:\n",
        "                job = bq.load_table_from_file(fh, dest, job_config=cfg, rewind=True)\n",
        "            _tqdmw(f\"â¬†ï¸  load {dest} rows {len(df)} (try {attempt})\")\n",
        "            job.result(timeout=300)\n",
        "            _tqdmw(f\"âœ… partition {yyyymm} rows {len(df)}\")\n",
        "            break\n",
        "        except (gex.GoogleAPICallError, gex.RetryError, TimeoutError) as e:\n",
        "            _tqdmw(f\"âš ï¸  {dest} {e} â€” backoff {backoff}s\")\n",
        "            time.sleep(backoff); backoff = min(backoff*2, 60)\n",
        "    _os.remove(tmp)\n",
        "\n",
        "def dedupe_partitions(parts: Set[int]):\n",
        "    \"\"\"De-dup within each yyyymm on (symbol, snapshotDate).\"\"\"\n",
        "    if not parts:\n",
        "        _tqdmw(\"ðŸ§¹ dedupe: no partitions to process\")\n",
        "        return\n",
        "\n",
        "    sql = f\"\"\"\n",
        "    -- Loop through each partition p\n",
        "    FOR p IN (SELECT part FROM UNNEST(@parts) AS part) DO\n",
        "\n",
        "      -- Build cleaned partition\n",
        "      CREATE OR REPLACE TEMP TABLE tmp AS\n",
        "      SELECT * EXCEPT(rn)\n",
        "      FROM (\n",
        "        SELECT\n",
        "          t.*,\n",
        "          ROW_NUMBER() OVER (\n",
        "            PARTITION BY symbol, snapshotDate\n",
        "            ORDER BY yyyymm DESC\n",
        "          ) AS rn\n",
        "        FROM `{table_ref}` AS t\n",
        "        WHERE yyyymm = p.part          -- âœ… use p.part here\n",
        "      )\n",
        "      WHERE rn = 1;\n",
        "\n",
        "      -- Replace existing rows for that partition\n",
        "      DELETE FROM `{table_ref}` WHERE yyyymm = p.part;  -- âœ… and here\n",
        "      INSERT INTO `{table_ref}` SELECT * FROM tmp;\n",
        "\n",
        "    END FOR;\n",
        "    \"\"\"\n",
        "\n",
        "    job = bq.query(\n",
        "        sql,\n",
        "        job_config=bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ArrayQueryParameter(\"parts\", \"INT64\", sorted(parts))\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "    job.result()\n",
        "    _tqdmw(f\"ðŸ§¹ dedupe: processed {len(parts)} partition(s)\")\n",
        "\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7) Pending buffers + autosave (locked) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_pending: Dict[int, List[pd.DataFrame]] = defaultdict(list)\n",
        "_pending_rows: Dict[int, int]           = defaultdict(int)\n",
        "_part_locks: Dict[int, asyncio.Lock]    = {}\n",
        "\n",
        "async def flush_partition(part: int):\n",
        "    lock = _part_locks.setdefault(part, asyncio.Lock())\n",
        "    async with lock:\n",
        "        if not _pending[part]:\n",
        "            return\n",
        "        batch = pd.concat(_pending[part])\n",
        "        _pending[part].clear(); _pending_rows[part] = 0\n",
        "        await asyncio.to_thread(load_to_bq, batch, part)\n",
        "\n",
        "async def flush_all(tag=\"ðŸ’¾ flush\"):\n",
        "    tasks = [flush_partition(p) for p,v in list(_pending.items()) if v]\n",
        "    if tasks:\n",
        "        _tqdmw(f\"{tag}: {len(tasks)} partitions\")\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "async def auto_flush():\n",
        "    while True:\n",
        "        await asyncio.sleep(AUTO_SAVE_SEC)\n",
        "        await flush_all(\"â³ autosave\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8) Main driver â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def main():\n",
        "    if not API_KEY:\n",
        "        raise SystemExit(\"âŒ  Set ALPHAVANTAGE_API_KEY and retry.\")\n",
        "\n",
        "    done = read_checkpoint()\n",
        "\n",
        "    connector = aiohttp.TCPConnector(limit=CONCURRENCY)\n",
        "    async with aiohttp.ClientSession(connector=connector,\n",
        "                                     timeout=aiohttp.ClientTimeout(total=90)) as session:\n",
        "        active, delisted = await asyncio.gather(\n",
        "            listing(session, \"active\"),\n",
        "            listing(session, \"delisted\")\n",
        "        )\n",
        "        _tqdmw(f\"â„¹ï¸ LISTING_STATUS â†’ active: {len(active):,}, delisted: {len(delisted):,}\")\n",
        "        universe = sorted(set(active + delisted))\n",
        "        if not universe:\n",
        "            _tqdmw(\"â— LISTING_STATUS returned 0; aborting (premium key expected to work).\")\n",
        "            return\n",
        "\n",
        "        todo = [s for s in universe if s not in done]\n",
        "        _tqdmw(f\"Universe {len(universe):,} | remaining {len(todo):,}\")\n",
        "\n",
        "        sem  = asyncio.Semaphore(CONCURRENCY)\n",
        "        pbar = tqdm.tqdm(total=len(todo), desc=\"Tickers processed\", dynamic_ncols=True)\n",
        "        saver = asyncio.create_task(auto_flush())\n",
        "        touched_parts: Set[int] = set()\n",
        "\n",
        "        async def worker(sym: str):\n",
        "            # Two AV calls per symbol; success-only checkpoint\n",
        "            async with sem:\n",
        "                try:\n",
        "                    bs = await fetch_balance_sheets(session, sym)\n",
        "                    if bs is None or bs.empty:\n",
        "                        mark_failure(sym); pbar.update(1); return\n",
        "\n",
        "                    prices = await fetch_price_series(session, sym)\n",
        "                    if prices is None or prices.empty:\n",
        "                        mark_failure(sym); pbar.update(1); return\n",
        "\n",
        "                    rows = []\n",
        "                    for _, r in bs.iterrows():\n",
        "                        report_date: dt.date = r[\"fiscalDateEnding\"]\n",
        "                        price_hit = price_on_or_before(prices, report_date)\n",
        "                        if not price_hit:\n",
        "                            continue\n",
        "                        snap_date, price = price_hit\n",
        "                        shares = float(r[\"commonStockSharesOutstanding\"])\n",
        "                        if pd.isna(shares) or pd.isna(price):  # skip bad rows\n",
        "                            continue\n",
        "                        rows.append({\n",
        "                            \"symbol\": sym,\n",
        "                            \"snapshotDate\": snap_date,\n",
        "                            \"reportedCurrency\": r[\"reportedCurrency\"],\n",
        "                            \"commonStockSharesOutstanding\": shares,\n",
        "                            \"adjustedClose\": float(price),\n",
        "                            \"marketCap\": float(price) * float(shares),\n",
        "                            \"yyyymm\": date_to_yyyymm(snap_date),\n",
        "                        })\n",
        "\n",
        "                    if rows:\n",
        "                        df = pd.DataFrame(rows).reindex(columns=CSV_COLS)\n",
        "                        for part, grp in df.groupby(\"yyyymm\"):\n",
        "                            _pending[part].append(grp)\n",
        "                            _pending_rows[part] += len(grp)\n",
        "                            touched_parts.add(int(part))\n",
        "                            # thresholded flush with lock\n",
        "                            if _pending_rows[part] >= FLUSH_ROWS:\n",
        "                                await flush_partition(part)\n",
        "                        await mark_done(sym)\n",
        "                    else:\n",
        "                        mark_failure(sym)\n",
        "                finally:\n",
        "                    pbar.update(1)\n",
        "\n",
        "        await asyncio.gather(*(asyncio.create_task(worker(s)) for s in todo))\n",
        "        await flush_all(\"âœ… final\")\n",
        "        saver.cancel()\n",
        "        pbar.close()\n",
        "\n",
        "    # End-of-run de-dup only for partitions we touched\n",
        "    await asyncio.to_thread(dedupe_partitions, touched_parts)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9) Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    # Jupyter/Colab friendly: ignore unknown argv\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "    asyncio.run(main())\n"
      ]
    }
  ]
}
