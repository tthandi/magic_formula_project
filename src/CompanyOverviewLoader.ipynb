{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB9+lWDd9rsecha9ETr/Ii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tthandi/magic_formula_project/blob/main/CompanyOverviewLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXFNA7sYGis"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "alpha_vantage_company_overview.py â€” COMPANY-OVERVIEW\n",
        "\n",
        "â€¢ Fetches Alpha Vantage OVERVIEW (company metadata) for US stocks (active + delisted).\n",
        "â€¢ Partitioned by yyyymm (month of fetch). Writes with WRITE_APPEND, then de-dups.\n",
        "â€¢ Modes:\n",
        "    full     â†’ all symbols\n",
        "    gapfill  â†’ symbols missing for current yyyymm\n",
        "    backfill â†’ symbols already present for current yyyymm (refresh), then de-dup\n",
        "    auto     â†’ full if current month empty else gapfill\n",
        "â€¢ Premium friendly: CALLS_PER_MIN=75, CONCURRENCY=50, async + rate-limited.\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROJECT   = \"\"\n",
        "DATASET   = \"\"\n",
        "TABLE     = \"company_overview\"\n",
        "\n",
        "import os as _os\n",
        "API_KEY   = '' # REQUIRED\n",
        "\n",
        "CALLS_PER_MIN = 75\n",
        "CONCURRENCY   = 50\n",
        "FLUSH_ROWS    = 5_000\n",
        "CHECKFILE     = \"av_done_symbols_co.txt\"\n",
        "FAILFILE      = \"av_failed_symbols_co.txt\"\n",
        "PART_START, PART_END = 198001, 204001  # yyyymm\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import io, csv, json, time, asyncio, tempfile, aiohttp, nest_asyncio, argparse, datetime\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set, Tuple\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "from aiolimiter import AsyncLimiter\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions as gex\n",
        "from google.auth.exceptions import DefaultCredentialsError, RefreshError\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Auth-robust BigQuery client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_bq_client() -> bigquery.Client:\n",
        "    # 1) Colab/Notebook user auth\n",
        "    try:\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "        colab_auth.authenticate_user()\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 2) Service account\n",
        "    sa_path = _os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    if sa_path and _os.path.exists(sa_path):\n",
        "        creds = service_account.Credentials.from_service_account_file(sa_path)\n",
        "        return bigquery.Client(project=PROJECT, credentials=creds)\n",
        "    # 3) ADC (gcloud)\n",
        "    try:\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except (DefaultCredentialsError, RefreshError) as e:\n",
        "        raise RuntimeError(\n",
        "            \"No Google Cloud credentials found. Use Colab auth, a service account \"\n",
        "            \"via GOOGLE_APPLICATION_CREDENTIALS, or `gcloud auth application-default login`.\"\n",
        "        ) from e\n",
        "\n",
        "bq = get_bq_client()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Alpha Vantage async helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_URL     = \"https://www.alphavantage.co/query\"\n",
        "RATE_LIMITER = AsyncLimiter(CALLS_PER_MIN, 60)\n",
        "\n",
        "async def av_request(session: aiohttp.ClientSession, params: dict) -> dict | None:\n",
        "    \"\"\"Generic AV GET with rate limiting & error handling.\"\"\"\n",
        "    async with RATE_LIMITER:\n",
        "        try:\n",
        "            async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                if resp.status != 200:\n",
        "                    return None\n",
        "                return await resp.json(content_type=None)\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "            return None\n",
        "\n",
        "async def listing(session: aiohttp.ClientSession, state: str, max_retries: int = 6) -> List[str]:\n",
        "    \"\"\"Robust LISTING_STATUS CSV â†’ list of stock symbols.\"\"\"\n",
        "    params = {\"function\": \"LISTING_STATUS\", \"state\": state, \"datatype\": \"csv\", \"apikey\": API_KEY}\n",
        "    backoff = 2\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        async with RATE_LIMITER:\n",
        "            try:\n",
        "                async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                    status = resp.status\n",
        "                    txt = await resp.text()\n",
        "            except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "                tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: network error (try {attempt})\")\n",
        "                await asyncio.sleep(backoff); backoff *= 2\n",
        "                continue\n",
        "\n",
        "        if txt.lstrip().startswith(\"{\"):\n",
        "            # Quota/info JSON even with 200 OK\n",
        "            try:\n",
        "                j = json.loads(txt)\n",
        "            except json.JSONDecodeError:\n",
        "                j = {}\n",
        "            msg = j.get(\"Note\") or j.get(\"Information\") or j.get(\"Error Message\") or f\"HTTP {status}\"\n",
        "            tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: {msg} (try {attempt})\")\n",
        "            await asyncio.sleep(backoff); backoff = min(backoff * 2, 30)\n",
        "            continue\n",
        "\n",
        "        rdr = csv.DictReader(io.StringIO(txt))\n",
        "        syms = [r[\"symbol\"] for r in rdr if r.get(\"assetType\") == \"Stock\" and r.get(\"symbol\")]\n",
        "        if syms:\n",
        "            return syms\n",
        "\n",
        "        tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: empty CSV (try {attempt})\")\n",
        "        await asyncio.sleep(backoff); backoff *= 2\n",
        "\n",
        "    tqdm.tqdm.write(f\"â— LISTING_STATUS {state}: giving up after {max_retries} attempts\")\n",
        "    return []\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Overview fetch & shaping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "TODAY        = datetime.date.today()\n",
        "FETCH_YYYYMM = TODAY.year * 100 + TODAY.month\n",
        "\n",
        "def coerce_numeric(col: pd.Series) -> pd.Series:\n",
        "    col = col.replace({\"None\": pd.NA, \"null\": pd.NA, \"\": pd.NA})\n",
        "    return pd.to_numeric(col, errors=\"coerce\")\n",
        "\n",
        "async def fetch_overview(session: aiohttp.ClientSession, sym: str) -> pd.DataFrame | None:\n",
        "    \"\"\"Return a one-row DataFrame with selected overview fields for *sym*.\"\"\"\n",
        "    data = await av_request(session, {\"function\": \"OVERVIEW\", \"symbol\": sym, \"apikey\": API_KEY})\n",
        "    # Alpha Vantage returns {} for unknown symbols\n",
        "    if not data or \"Symbol\" not in data:\n",
        "        return None\n",
        "\n",
        "    keep_map = {\n",
        "        \"Symbol\": \"symbol\",\n",
        "        \"MarketCapitalization\": \"marketCap\",\n",
        "        \"PERatio\": \"peRatio\",\n",
        "        \"Sector\": \"sector\",\n",
        "        \"Industry\": \"industry\",\n",
        "        \"Country\": \"country\",\n",
        "    }\n",
        "    row = {new: data.get(old) for old, new in keep_map.items()}\n",
        "    row[\"yyyymm\"] = FETCH_YYYYMM\n",
        "    df = pd.DataFrame([row])\n",
        "    # numeric casts\n",
        "    for c in [\"marketCap\", \"peRatio\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = coerce_numeric(df[c])\n",
        "    return df\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BigQuery boot-strapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Dataset\n",
        "try:\n",
        "    bq.get_dataset(f\"{PROJECT}.{DATASET}\")\n",
        "except gex.NotFound:\n",
        "    bq.create_dataset(bigquery.Dataset(f\"{PROJECT}.{DATASET}\"))\n",
        "\n",
        "table_ref = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "SCHEMA = [\n",
        "    bigquery.SchemaField(\"symbol\",    \"STRING\"),\n",
        "    bigquery.SchemaField(\"marketCap\", \"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"peRatio\",   \"FLOAT64\"),\n",
        "    bigquery.SchemaField(\"sector\",    \"STRING\"),\n",
        "    bigquery.SchemaField(\"industry\",  \"STRING\"),\n",
        "    bigquery.SchemaField(\"country\",   \"STRING\"),\n",
        "    bigquery.SchemaField(\"yyyymm\",    \"INT64\"),\n",
        "]\n",
        "\n",
        "def ensure_table():\n",
        "    try:\n",
        "        tbl = bq.get_table(table_ref)\n",
        "        existing = {f.name for f in tbl.schema}\n",
        "        need = [f for f in SCHEMA if f.name not in existing]\n",
        "        if need:\n",
        "            tbl.schema += need\n",
        "            bq.update_table(tbl, [\"schema\"])\n",
        "            tqdm.tqdm.write(f\"ðŸ†• added missing columns â†’ {', '.join(f.name for f in need)}\")\n",
        "    except gex.NotFound:\n",
        "        tbl = bigquery.Table(table_ref, schema=SCHEMA)\n",
        "        tbl.range_partitioning = bigquery.RangePartitioning(\n",
        "            field=\"yyyymm\", range_=bigquery.PartitionRange(PART_START, PART_END, 1)\n",
        "        )\n",
        "        tbl.clustering_fields = [\"symbol\"]\n",
        "        bq.create_table(tbl)\n",
        "        tqdm.tqdm.write(f\"âž¡ï¸  created table {table_ref}\")\n",
        "\n",
        "ensure_table()\n",
        "CSV_COLS = [f.name for f in bq.get_table(table_ref).schema]  # keep order consistent\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load & Dedupe helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def load_partition(df_part: pd.DataFrame, yyyymm: int, retries: int = 4):\n",
        "    \"\"\"Append df_part to TABLE$yyyymm via gzip CSV.\"\"\"\n",
        "    if df_part is None or df_part.empty:\n",
        "        return\n",
        "    df_part = df_part.reindex(columns=CSV_COLS)\n",
        "    fd, tmp = tempfile.mkstemp(suffix=\".csv.gz\"); _os.close(fd)\n",
        "    df_part.to_csv(tmp, index=False, compression=\"gzip\", date_format=\"%Y-%m-%d\")\n",
        "\n",
        "    dest = f\"{table_ref}${yyyymm}\"  # explicitly target this partition\n",
        "    cfg = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "    )\n",
        "    backoff = 5\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            with open(tmp, \"rb\") as fh:\n",
        "                job = bq.load_table_from_file(fh, dest, job_config=cfg, rewind=True)\n",
        "            tqdm.tqdm.write(f\"â¬†ï¸  load {dest} rows {len(df_part)} (try {attempt})\")\n",
        "            job.result(timeout=300)\n",
        "            tqdm.tqdm.write(f\"âœ… partition {yyyymm} rows {len(df_part)}\")\n",
        "            break\n",
        "        except (gex.GoogleAPICallError, gex.RetryError, TimeoutError) as e:\n",
        "            tqdm.tqdm.write(f\"âš ï¸  {dest} {e} â€” backoff {backoff}s\")\n",
        "            time.sleep(backoff); backoff = min(backoff * 2, 60)\n",
        "    _os.remove(tmp)\n",
        "\n",
        "def dedupe_month(yyyymm: int):\n",
        "    \"\"\"Keep one row per (symbol, yyyymm).\"\"\"\n",
        "    sql = f\"\"\"\n",
        "    CREATE OR REPLACE TEMP TABLE tmp AS\n",
        "    SELECT * EXCEPT(rn) FROM (\n",
        "      SELECT t.*,\n",
        "             ROW_NUMBER() OVER (\n",
        "               PARTITION BY symbol, yyyymm\n",
        "               ORDER BY symbol\n",
        "             ) rn\n",
        "      FROM `{table_ref}` t\n",
        "      WHERE yyyymm = @p\n",
        "    )\n",
        "    WHERE rn = 1;\n",
        "\n",
        "    DELETE FROM `{table_ref}` WHERE yyyymm = @p;\n",
        "    INSERT INTO `{table_ref}` SELECT * FROM tmp;\n",
        "    \"\"\"\n",
        "    job = bq.query(\n",
        "        sql,\n",
        "        job_config=bigquery.QueryJobConfig(\n",
        "            query_parameters=[bigquery.ScalarQueryParameter(\"p\", \"INT64\", yyyymm)]\n",
        "        ),\n",
        "    )\n",
        "    job.result()\n",
        "    tqdm.tqdm.write(f\"ðŸ§¹ dedupe complete for {yyyymm}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Checkpoint / symbols â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def read_checkpoint() -> Set[str]:\n",
        "    return set(open(CHECKFILE).read().splitlines()) if _os.path.exists(CHECKFILE) else set()\n",
        "\n",
        "def add_checkpoint(sym: str):\n",
        "    with open(CHECKFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "def add_failure(sym: str):\n",
        "    with open(FAILFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "def symbols_loaded_for_month(yyyymm: int) -> Set[str]:\n",
        "    q = f\"SELECT DISTINCT symbol FROM `{table_ref}` WHERE yyyymm = @p\"\n",
        "    rows = bq.query(q, job_config=bigquery.QueryJobConfig(\n",
        "        query_parameters=[bigquery.ScalarQueryParameter(\"p\", \"INT64\", yyyymm)]\n",
        "    )).result()\n",
        "    return {r.symbol for r in rows}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Mode selection helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def decide_mode(requested: str, existing_for_month: Set[str], universe: List[str]) -> str:\n",
        "    if requested != \"auto\":\n",
        "        return requested\n",
        "    if not existing_for_month:\n",
        "        tqdm.tqdm.write(\"ðŸ§­ Auto: current month empty â†’ FULL\")\n",
        "        return \"full\"\n",
        "    tqdm.tqdm.write(\"ðŸ§­ Auto: current month has data â†’ GAP-FILL\")\n",
        "    return \"gapfill\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main async entry-point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def run(mode: str):\n",
        "    if not API_KEY:\n",
        "        raise SystemExit(\"âŒ Set ALPHAVANTAGE_API_KEY and retry.\")\n",
        "\n",
        "    pending: List[pd.DataFrame] = []\n",
        "    sem       = asyncio.Semaphore(CONCURRENCY)\n",
        "    connector = aiohttp.TCPConnector(limit=CONCURRENCY)\n",
        "\n",
        "    async with aiohttp.ClientSession(connector=connector,\n",
        "                                     timeout=aiohttp.ClientTimeout(total=90)) as session:\n",
        "        active, delisted = await asyncio.gather(\n",
        "            listing(session, \"active\"),\n",
        "            listing(session, \"delisted\"),\n",
        "        )\n",
        "        tqdm.tqdm.write(f\"â„¹ï¸ LISTING_STATUS â†’ active: {len(active):,}, delisted: {len(delisted):,}\")\n",
        "        universe = sorted(set(active + delisted))\n",
        "        if not universe:\n",
        "            tqdm.tqdm.write(\"â— LISTING_STATUS returned 0; aborting (premium key expected to work).\")\n",
        "            return\n",
        "\n",
        "        existing_month = symbols_loaded_for_month(FETCH_YYYYMM)\n",
        "        mode = decide_mode(mode, existing_month, universe)\n",
        "\n",
        "        if mode == \"full\":\n",
        "            todo = universe\n",
        "            desc = \"FULL crawl (overview)\"\n",
        "        elif mode == \"gapfill\":\n",
        "            todo = [s for s in universe if s not in existing_month]\n",
        "            desc = \"GAP-FILL (missing for current month)\"\n",
        "        elif mode == \"backfill\":\n",
        "            todo = [s for s in universe if s in existing_month]\n",
        "            desc = \"BACKFILL (already present this month)\"\n",
        "        else:\n",
        "            raise SystemExit(f\"Unknown mode: {mode}\")\n",
        "\n",
        "        # (Optional) also factor in historical success checkpoint if you want resume semantics\n",
        "        done = read_checkpoint()\n",
        "        # Only skip by checkpoint for full/backfill; for gapfill we already filtered by BQ presence\n",
        "        if mode in (\"full\", \"backfill\"):\n",
        "            todo = [s for s in todo if s not in done]\n",
        "\n",
        "        tqdm.tqdm.write(f\"Mode: {desc} | universe {len(universe):,} | \"\n",
        "                        f\"in-month {len(existing_month):,} | to fetch {len(todo):,}\")\n",
        "\n",
        "        pbar = tqdm.tqdm(total=len(todo), desc=\"Symbols processed\", dynamic_ncols=True)\n",
        "\n",
        "        async def worker(sym: str):\n",
        "            async with sem:\n",
        "                df = await fetch_overview(session, sym)\n",
        "                if df is not None and not df.empty:\n",
        "                    pending.append(df)\n",
        "                    # flush on threshold (single month partition)\n",
        "                    if sum(len(d) for d in pending) >= FLUSH_ROWS:\n",
        "                        batch = pd.concat(pending); pending.clear()\n",
        "                        await asyncio.to_thread(load_partition, batch, FETCH_YYYYMM)\n",
        "                    add_checkpoint(sym)  # success-only\n",
        "                else:\n",
        "                    add_failure(sym)\n",
        "                pbar.update(1)\n",
        "\n",
        "        await asyncio.gather(*(asyncio.create_task(worker(s)) for s in todo))\n",
        "        pbar.close()\n",
        "\n",
        "    # Final flush\n",
        "    if pending:\n",
        "        await asyncio.to_thread(load_partition, pd.concat(pending), FETCH_YYYYMM)\n",
        "\n",
        "    # De-dup the current month\n",
        "    await asyncio.to_thread(dedupe_month, FETCH_YYYYMM)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", choices=[\"auto\", \"full\", \"gapfill\", \"backfill\"],\n",
        "                        default=\"auto\",\n",
        "                        help=\"auto=full if current month empty else gap-fill\")\n",
        "    args, _unknown = parser.parse_known_args()  # Jupyter-friendly\n",
        "    asyncio.run(run(args.mode))\n"
      ]
    }
  ]
}
