{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRDkzwQw8TrLtDRXUXs0Y7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tthandi/magic_formula_project/blob/main/IncomeStatementLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PnJ2V03X5RW"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "alpha_vantage_income_statement_full.py â€” INCOME-STATEMENT\n",
        "\n",
        "â€¢ Quarterly income statements for US stocks (active + delisted).\n",
        "â€¢ Modes:\n",
        "    full     â†’ all symbols\n",
        "    gapfill  â†’ symbols missing from BQ\n",
        "    backfill â†’ symbols already in BQ (row-level backfill; end-of-run de-dup)\n",
        "â€¢ Premium-friendly: CALLS_PER_MIN=75, CONCURRENCY=50, robust async LISTING_STATUS.\n",
        "â€¢ Dynamic schema discovery + auto-evolve (incl. mid-run).\n",
        "â€¢ Safe CSV loads (column-order alignment, mkstemp, WRITE_APPEND to partitions).\n",
        "â€¢ Success-only checkpoint + failure log.\n",
        "â€¢ Per-partition flush locks; end-of-run de-dup on (symbol, fiscalDateEnding).\n",
        "â€¢ Jupyter-friendly CLI (parse_known_args).\n",
        "\n",
        "Env:\n",
        "  ALPHAVANTAGE_API_KEY  (required)\n",
        "  GOOGLE_APPLICATION_CREDENTIALS (optional, path to service account JSON)\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROJECT   = \"\"\n",
        "DATASET   = \"\"\n",
        "TABLE     = \"income_statement\"   # same table name, now quarterly data\n",
        "\n",
        "import os as _os\n",
        "API_KEY   = '' # REQUIRED\n",
        "\n",
        "CALLS_PER_MIN = 75\n",
        "CONCURRENCY   = 50\n",
        "FLUSH_ROWS    = 10_000\n",
        "CHECKFILE     = \"av_done_symbols_is.txt\"\n",
        "FAILFILE      = \"av_failed_symbols_is.txt\"\n",
        "PART_START, PART_END = 198001, 204001  # yyyymm partition range\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import io, csv, json, time, asyncio, tempfile, aiohttp, nest_asyncio, argparse\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set, Tuple\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tqdm                      # module-style import\n",
        "from aiolimiter import AsyncLimiter\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions as gex\n",
        "from google.auth.exceptions import DefaultCredentialsError, RefreshError\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Auth-robust BigQuery client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_bq_client() -> bigquery.Client:\n",
        "    try:\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "        colab_auth.authenticate_user()\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except Exception:\n",
        "        pass\n",
        "    sa_path = _os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    if sa_path and _os.path.exists(sa_path):\n",
        "        creds = service_account.Credentials.from_service_account_file(sa_path)\n",
        "        return bigquery.Client(project=PROJECT, credentials=creds)\n",
        "    try:\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except (DefaultCredentialsError, RefreshError) as e:\n",
        "        raise RuntimeError(\n",
        "            \"No Google Cloud credentials found. Do one of:\\n\"\n",
        "            \"  â€¢ Colab: from google.colab import auth; auth.authenticate_user()\\n\"\n",
        "            \"  â€¢ Service account: set GOOGLE_APPLICATION_CREDENTIALS to your JSON path\\n\"\n",
        "            \"  â€¢ gcloud: run `gcloud auth application-default login`\"\n",
        "        ) from e\n",
        "\n",
        "bq = get_bq_client()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Alpha Vantage helpers (async) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_URL     = \"https://www.alphavantage.co/query\"\n",
        "RATE_LIMITER = AsyncLimiter(CALLS_PER_MIN, 60)\n",
        "\n",
        "async def av_request(session: aiohttp.ClientSession, params: dict) -> dict | None:\n",
        "    async with RATE_LIMITER:\n",
        "        try:\n",
        "            async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                if resp.status != 200:\n",
        "                    return None\n",
        "                return await resp.json(content_type=None)\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "            return None\n",
        "\n",
        "async def listing(session: aiohttp.ClientSession, state: str, max_retries: int = 6) -> List[str]:\n",
        "    params = {\"function\": \"LISTING_STATUS\", \"state\": state, \"datatype\": \"csv\", \"apikey\": API_KEY}\n",
        "    backoff = 2\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        async with RATE_LIMITER:\n",
        "            try:\n",
        "                async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                    status = resp.status\n",
        "                    txt = await resp.text()\n",
        "            except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "                tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: network error (try {attempt})\")\n",
        "                await asyncio.sleep(backoff); backoff *= 2\n",
        "                continue\n",
        "        if txt.lstrip().startswith(\"{\"):\n",
        "            try:\n",
        "                j = json.loads(txt)\n",
        "            except json.JSONDecodeError:\n",
        "                j = {}\n",
        "            msg = j.get(\"Note\") or j.get(\"Information\") or j.get(\"Error Message\") or f\"HTTP {status}\"\n",
        "            tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: {msg} (try {attempt})\")\n",
        "            await asyncio.sleep(backoff); backoff = min(backoff * 2, 30)\n",
        "            continue\n",
        "        rdr = csv.DictReader(io.StringIO(txt))\n",
        "        syms = [r[\"symbol\"] for r in rdr if r.get(\"assetType\") == \"Stock\" and r.get(\"symbol\")]\n",
        "        if syms:\n",
        "            return syms\n",
        "        tqdm.tqdm.write(f\"âš ï¸ LISTING_STATUS {state}: empty CSV (try {attempt})\")\n",
        "        await asyncio.sleep(backoff); backoff *= 2\n",
        "    tqdm.tqdm.write(f\"â— LISTING_STATUS {state}: giving up after {max_retries} attempts\")\n",
        "    return []\n",
        "\n",
        "def coerce_numeric(col: pd.Series) -> pd.Series:\n",
        "    col = col.replace({\"None\": pd.NA, \"null\": pd.NA, \"\": pd.NA})\n",
        "    return pd.to_numeric(col, errors=\"coerce\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ fetch QUARTERLY income statement here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def fetch_income_statement(session: aiohttp.ClientSession, sym: str) -> pd.DataFrame | None:\n",
        "    data = await av_request(\n",
        "        session,\n",
        "        {\"function\": \"INCOME_STATEMENT\", \"symbol\": sym, \"apikey\": API_KEY}\n",
        "    )\n",
        "    # use quarterlyReports instead of annualReports\n",
        "    if not data or not data.get(\"quarterlyReports\"):\n",
        "        return None\n",
        "\n",
        "    df = pd.DataFrame(data[\"quarterlyReports\"])\n",
        "    df[\"symbol\"] = sym\n",
        "\n",
        "    # SG&A alias normalization\n",
        "    if \"sellingGeneralAndAdministrative\" in df.columns and \"sellingGeneralAdministrative\" not in df.columns:\n",
        "        df[\"sellingGeneralAdministrative\"] = df[\"sellingGeneralAndAdministrative\"]\n",
        "\n",
        "    df[\"fiscalDateEnding\"] = pd.to_datetime(df[\"fiscalDateEnding\"]).dt.date\n",
        "    df[\"yyyymm\"] = df[\"fiscalDateEnding\"].apply(lambda d: d.year * 100 + d.month).astype(\"int64\")\n",
        "\n",
        "    non_numeric = {\"symbol\", \"fiscalDateEnding\", \"reportedCurrency\", \"yyyymm\"}\n",
        "    numeric_cols = [c for c in df.columns if c not in non_numeric]\n",
        "    if numeric_cols:\n",
        "        df[numeric_cols] = df[numeric_cols].apply(coerce_numeric)\n",
        "    return df\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BigQuery boot-strapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    bq.get_dataset(f\"{PROJECT}.{DATASET}\")\n",
        "except gex.NotFound:\n",
        "    bq.create_dataset(bigquery.Dataset(f\"{PROJECT}.{DATASET}\"))\n",
        "\n",
        "table_ref = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "\n",
        "SAMPLE_TICKERS = [\"AAPL\", \"IBM\", \"MSFT\", \"XOM\", \"GE\", \"GOOGL\", \"AMZN\", \"BAC\"]\n",
        "\n",
        "def discover_fields(sample_syms: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Discover all possible fields from both annual and quarterly income statements\n",
        "    for a small sample of symbols, so our schema covers everything upfront.\n",
        "    \"\"\"\n",
        "    fields: Set[str] = set()\n",
        "    for sym in sample_syms:\n",
        "        try:\n",
        "            resp = requests.get(\n",
        "                BASE_URL,\n",
        "                params={\"function\": \"INCOME_STATEMENT\", \"symbol\": sym, \"apikey\": API_KEY},\n",
        "                timeout=20\n",
        "            ).json()\n",
        "            # include both, in case quarterlyReports has extra keys\n",
        "            fields |= {k for rpt in resp.get(\"annualReports\", []) for k in rpt}\n",
        "            fields |= {k for rpt in resp.get(\"quarterlyReports\", []) for k in rpt}\n",
        "        except Exception:\n",
        "            continue\n",
        "        if len(fields) >= 80:\n",
        "            break\n",
        "    if \"sellingGeneralAndAdministrative\" in fields and \"sellingGeneralAdministrative\" not in fields:\n",
        "        fields.add(\"sellingGeneralAdministrative\")\n",
        "    base = [\"symbol\", \"fiscalDateEnding\", \"reportedCurrency\", \"yyyymm\"]\n",
        "    return base + sorted(fields - set(base))\n",
        "\n",
        "FIELDS = discover_fields(SAMPLE_TICKERS)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FLOAT fix for problematic numeric fields â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_TYPE_OVERRIDES = {\n",
        "    \"symbol\": \"STRING\",\n",
        "    \"fiscalDateEnding\": \"DATE\",\n",
        "    \"reportedCurrency\": \"STRING\",\n",
        "    \"yyyymm\": \"INT64\",\n",
        "    # IMPORTANT: use FLOAT64 to avoid CSV \"1234.0\" â†’ INT64 parse errors\n",
        "    \"commonStockSharesOutstanding\": \"FLOAT64\",\n",
        "}\n",
        "\n",
        "SCHEMA = [bigquery.SchemaField(col, _TYPE_OVERRIDES.get(col, \"FLOAT64\")) for col in FIELDS]\n",
        "\n",
        "def ensure_table() -> Tuple[bool, List[str]]:\n",
        "    try:\n",
        "        tbl = bq.get_table(table_ref)\n",
        "        existing = {f.name for f in tbl.schema}\n",
        "        missing = [c for c in FIELDS if c not in existing]\n",
        "        if missing:\n",
        "            tbl.schema += [bigquery.SchemaField(c, _TYPE_OVERRIDES.get(c, \"FLOAT64\")) for c in missing]\n",
        "            bq.update_table(tbl, [\"schema\"])\n",
        "            tqdm.tqdm.write(\n",
        "                f\"ğŸ†• Added {len(missing)} new columns â†’ {', '.join(missing[:6])}\"\n",
        "                f\"{'â€¦' if len(missing)>6 else ''}\"\n",
        "            )\n",
        "            return True, missing\n",
        "        return False, []\n",
        "    except gex.NotFound:\n",
        "        tbl = bigquery.Table(table_ref, schema=SCHEMA)\n",
        "        tbl.range_partitioning = bigquery.RangePartitioning(\n",
        "            field=\"yyyymm\", range_=bigquery.PartitionRange(PART_START, PART_END, 1)\n",
        "        )\n",
        "        tbl.clustering_fields = [\"symbol\"]\n",
        "        bq.create_table(tbl)\n",
        "        tqdm.tqdm.write(f\"â¡ï¸  created table {table_ref} with {len(SCHEMA)} columns\")\n",
        "        return True, [f.name for f in tbl.schema]\n",
        "\n",
        "schema_changed_initial, _ = ensure_table()\n",
        "CSV_COLS = [f.name for f in bq.get_table(table_ref).schema]\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Mid-run schema evolve if new columns appear â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "_schema_lock = asyncio.Lock()\n",
        "async def ensure_columns_midrun(new_cols: Set[str]) -> bool:\n",
        "    if not new_cols:\n",
        "        return False\n",
        "    async with _schema_lock:\n",
        "        tbl = bq.get_table(table_ref)\n",
        "        existing = {f.name for f in tbl.schema}\n",
        "        to_add = [c for c in new_cols if c not in existing]\n",
        "        if not to_add:\n",
        "            return False\n",
        "        tbl.schema += [bigquery.SchemaField(c, _TYPE_OVERRIDES.get(c, \"FLOAT64\")) for c in to_add]\n",
        "        bq.update_table(tbl, [\"schema\"])\n",
        "        global CSV_COLS\n",
        "        CSV_COLS = [f.name for f in bq.get_table(table_ref).schema]\n",
        "        tqdm.tqdm.write(f\"ğŸ†• (mid-run) added {len(to_add)} cols â†’ {', '.join(to_add)}\")\n",
        "        return True\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load helper (partitioned) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def load_partition(df_part: pd.DataFrame, yyyymm: int, retries: int = 4):\n",
        "    if df_part is None or df_part.empty:\n",
        "        return\n",
        "    df_part = df_part.reindex(columns=CSV_COLS)\n",
        "    fd, tmp = tempfile.mkstemp(suffix=\".csv.gz\"); _os.close(fd)\n",
        "    df_part.to_csv(tmp, index=False, compression=\"gzip\", date_format=\"%Y-%m-%d\")\n",
        "    dest = f\"{table_ref}${yyyymm}\"\n",
        "    cfg = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "    )\n",
        "    backoff = 5\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            with open(tmp, \"rb\") as fh:\n",
        "                job = bq.load_table_from_file(fh, dest, job_config=cfg, rewind=True)\n",
        "            tqdm.tqdm.write(f\"â¬†ï¸  load {dest} rows {len(df_part)} (try {attempt})\")\n",
        "            job.result(timeout=300)\n",
        "            tqdm.tqdm.write(f\"âœ… partition {yyyymm} rows {len(df_part)}\")\n",
        "            break\n",
        "        except (gex.GoogleAPICallError, gex.RetryError, TimeoutError) as e:\n",
        "            tqdm.tqdm.write(f\"âš ï¸  {dest} {e} â€” backoff {backoff}s\")\n",
        "            time.sleep(backoff); backoff = min(backoff * 2, 60)\n",
        "    _os.remove(tmp)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Checkpoint / failure helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def read_checkpoint() -> Set[str]:\n",
        "    return set(open(CHECKFILE).read().splitlines()) if _os.path.exists(CHECKFILE) else set()\n",
        "\n",
        "def add_checkpoint(sym: str):\n",
        "    with open(CHECKFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "def add_failure(sym: str):\n",
        "    with open(FAILFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ De-duplication helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def dedupe_partitions(parts: Set[int]):\n",
        "    import tqdm as _tqdm\n",
        "    if not parts:\n",
        "        _tqdm.tqdm.write(\"ğŸ§¹ dedupe: no partitions to process\")\n",
        "        return\n",
        "\n",
        "    sql = f\"\"\"\n",
        "    FOR part IN (SELECT p FROM UNNEST(@parts) AS p) DO\n",
        "      CREATE OR REPLACE TEMP TABLE tmp AS\n",
        "      SELECT * EXCEPT(rn) FROM (\n",
        "        SELECT t.*,\n",
        "               ROW_NUMBER() OVER (\n",
        "                 PARTITION BY symbol, fiscalDateEnding\n",
        "                 ORDER BY yyyymm DESC\n",
        "               ) AS rn\n",
        "        FROM `{table_ref}` AS t\n",
        "        WHERE yyyymm = part.p      -- âœ… use part.p (struct field)\n",
        "      )\n",
        "      WHERE rn = 1;\n",
        "\n",
        "      DELETE FROM `{table_ref}` WHERE yyyymm = part.p;   -- âœ… use part.p\n",
        "      INSERT INTO `{table_ref}` SELECT * FROM tmp;\n",
        "    END FOR;\n",
        "    \"\"\"\n",
        "\n",
        "    job = bq.query(\n",
        "        sql,\n",
        "        job_config=bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ArrayQueryParameter(\"parts\", \"INT64\", sorted(parts))\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "    job.result()\n",
        "    _tqdm.tqdm.write(f\"ğŸ§¹ dedupe: processed {len(parts)} partition(s)\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main async entrypoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def run(mode: str):\n",
        "    if not API_KEY:\n",
        "        raise SystemExit(\"âŒ  Set ALPHAVANTAGE_API_KEY and retry.\")\n",
        "\n",
        "    sem                         = asyncio.Semaphore(CONCURRENCY)\n",
        "    pending: Dict[int, List[pd.DataFrame]] = defaultdict(list)\n",
        "    pending_rows: Dict[int, int]           = defaultdict(int)\n",
        "    part_locks: Dict[int, asyncio.Lock]    = {}\n",
        "    touched_parts: Set[int]                = set()\n",
        "    schema_changed_flag: bool              = schema_changed_initial\n",
        "\n",
        "    async def maybe_flush(part: int):\n",
        "        lock = part_locks.setdefault(part, asyncio.Lock())\n",
        "        async with lock:\n",
        "            if pending_rows[part] >= FLUSH_ROWS:\n",
        "                batch = pd.concat(pending[part]); pending[part].clear(); pending_rows[part] = 0\n",
        "                await asyncio.to_thread(load_partition, batch, part)\n",
        "\n",
        "    connector = aiohttp.TCPConnector(limit=CONCURRENCY)\n",
        "    async with aiohttp.ClientSession(\n",
        "        connector=connector,\n",
        "        timeout=aiohttp.ClientTimeout(total=90)\n",
        "    ) as session:\n",
        "        active, delisted = await asyncio.gather(\n",
        "            listing(session, \"active\"),\n",
        "            listing(session, \"delisted\")\n",
        "        )\n",
        "        tqdm.tqdm.write(\n",
        "            f\"â„¹ï¸  LISTING_STATUS â†’ active: {len(active):,}, delisted: {len(delisted):,}\"\n",
        "        )\n",
        "        universe = sorted(set(active + delisted))\n",
        "        if not universe:\n",
        "            tqdm.tqdm.write(\"â— LISTING_STATUS returned 0; aborting (premium key expected to work).\")\n",
        "            return\n",
        "\n",
        "        # Build mode-specific TODO\n",
        "        existing = {\n",
        "            row.symbol\n",
        "            for row in bq.query(f\"SELECT DISTINCT symbol FROM `{table_ref}`\").result()\n",
        "        }\n",
        "        if mode == \"auto\":\n",
        "            mode = \"full\" if len(existing) == 0 else \"gapfill\"\n",
        "\n",
        "        if mode == \"full\":\n",
        "            todo = universe\n",
        "            todo_desc = \"FULL crawl (quarterly)\"\n",
        "        elif mode == \"gapfill\":\n",
        "            todo = [s for s in universe if s not in existing]\n",
        "            todo_desc = \"GAP-FILL (missing symbols only, quarterly)\"\n",
        "        elif mode == \"backfill\":\n",
        "            todo = [s for s in universe if s in existing]\n",
        "            todo_desc = \"BACKFILL (existing symbols only, quarterly)\"\n",
        "        else:\n",
        "            raise SystemExit(f\"Unknown mode: {mode}\")\n",
        "\n",
        "        tqdm.tqdm.write(\n",
        "            f\"Mode: {todo_desc} | universe {len(universe):,} | \"\n",
        "            f\"already in BQ {len(existing):,} | to fetch {len(todo):,}\"\n",
        "        )\n",
        "\n",
        "        pbar = tqdm.tqdm(total=len(todo), desc=\"Symbols processed\", dynamic_ncols=True)\n",
        "\n",
        "        async def worker(sym: str):\n",
        "            nonlocal schema_changed_flag\n",
        "            async with sem:\n",
        "                df = await fetch_income_statement(session, sym)\n",
        "                if df is None or df.empty:\n",
        "                    add_failure(sym)\n",
        "                    pbar.update(1)\n",
        "                    return\n",
        "\n",
        "                new_cols = set(df.columns) - set(CSV_COLS)\n",
        "                if new_cols:\n",
        "                    changed = await ensure_columns_midrun(new_cols)\n",
        "                    schema_changed_flag = schema_changed_flag or changed\n",
        "\n",
        "                for part, grp in df.groupby(\"yyyymm\"):\n",
        "                    pending[part].append(grp)\n",
        "                    pending_rows[part] += len(grp)\n",
        "                    touched_parts.add(int(part))\n",
        "                    await maybe_flush(part)\n",
        "\n",
        "                add_checkpoint(sym)  # success-only\n",
        "                pbar.update(1)\n",
        "\n",
        "        tasks = [asyncio.create_task(worker(s)) for s in todo]\n",
        "        await asyncio.gather(*tasks)\n",
        "        pbar.close()\n",
        "\n",
        "    # flush any remaining\n",
        "    for part, dfs in pending.items():\n",
        "        if dfs:\n",
        "            await asyncio.to_thread(load_partition, pd.concat(dfs), part)\n",
        "\n",
        "    # de-dup quarterly rows per (symbol, fiscalDateEnding)\n",
        "    await asyncio.to_thread(dedupe_partitions, touched_parts)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        choices=[\"auto\", \"full\", \"gapfill\", \"backfill\"],\n",
        "        default=\"auto\",\n",
        "        help=\"auto=full if empty else gap-fill; backfill=existing symbols only\",\n",
        "    )\n",
        "    args, _unknown = parser.parse_known_args()\n",
        "    asyncio.run(run(args.mode))\n"
      ]
    }
  ]
}
