{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx7vCxVR2S8M/3HK/2ZmHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tthandi/magic_formula_project/blob/main/AdjustedDailyPriceLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrrnT1NAXUzM"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "alpha_vantage_daily_price_auto.py  â€“  full-crawl OR gap-fill (auto)\n",
        "\n",
        "Behavior\n",
        "--------\n",
        "* If the destination table has **no symbols**, run a **full crawl** (all AV symbols).\n",
        "* If the table already has data, run a **gap-fill** for **missing** symbols only.\n",
        "* Safe + fast: async fetch, rate-limited, WRITE_APPEND, per-partition flush locks.\n",
        "* End-of-run de-dup cleans only the partitions we touched.\n",
        "\n",
        "Auth\n",
        "----\n",
        "* Works in Colab (user auth), with a service-account JSON via\n",
        "  GOOGLE_APPLICATION_CREDENTIALS, or with gcloud ADC.\n",
        "\n",
        "Env\n",
        "---\n",
        "* Alpha Vantage key via ALPHAVANTAGE_API_KEY (falls back to literal if unset).\n",
        "\"\"\"\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "PROJECT   = \"\"      # GCP project ID\n",
        "DATASET   = \"\"      # BigQuery dataset\n",
        "TABLE     = \"\"      # Destination table name\n",
        "CHECKFILE = \"av_done_symbols.txt\"           # success-only (not used to skip in full mode)\n",
        "FAILFILE  = \"av_failed_symbols.txt\"         # failures recorded here\n",
        "\n",
        "import os as _os\n",
        "API_KEY   = _os.getenv(\"ALPHAVANTAGE_API_KEY\") or \"\"\n",
        "\n",
        "CALLS_PER_MIN  = 75                         # paid-tier limit (free=5)\n",
        "CONCURRENCY    = 50                         # concurrent HTTP requests\n",
        "FLUSH_ROWS     = 100_000                    # per-partition flush threshold\n",
        "PART_START, PART_END = 199901, 204001       # RANGE partition yyyymm\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import io, csv, time, asyncio, tempfile, aiohttp, nest_asyncio, sys\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Set\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "from aiolimiter import AsyncLimiter\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.api_core import exceptions as gex\n",
        "from google.auth.exceptions import DefaultCredentialsError, RefreshError\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Auth-robust BigQuery client â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_bq_client() -> bigquery.Client:\n",
        "    # 1) Colab / Notebook user auth\n",
        "    try:\n",
        "        from google.colab import auth as colab_auth  # type: ignore\n",
        "        colab_auth.authenticate_user()\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Service account via GOOGLE_APPLICATION_CREDENTIALS\n",
        "    sa_path = _os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    if sa_path and _os.path.exists(sa_path):\n",
        "        creds = service_account.Credentials.from_service_account_file(sa_path)\n",
        "        return bigquery.Client(project=PROJECT, credentials=creds)\n",
        "\n",
        "    # 3) Application Default Credentials (gcloud)\n",
        "    #    Make sure you've run:  gcloud auth application-default login\n",
        "    try:\n",
        "        return bigquery.Client(project=PROJECT)\n",
        "    except (DefaultCredentialsError, RefreshError) as e:\n",
        "        raise RuntimeError(\n",
        "            \"No Google Cloud credentials found. Do one of:\\n\"\n",
        "            \"  â€¢ In Colab: from google.colab import auth; auth.authenticate_user()\\n\"\n",
        "            \"  â€¢ With service account: set GOOGLE_APPLICATION_CREDENTIALS to your JSON path\\n\"\n",
        "            \"  â€¢ With gcloud: run `gcloud auth application-default login`\"\n",
        "        ) from e\n",
        "\n",
        "bq = get_bq_client()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Alpha Vantage async fetchers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "BASE_URL     = \"https://www.alphavantage.co/query\"\n",
        "RATE_LIMITER = AsyncLimiter(CALLS_PER_MIN, 60)\n",
        "\n",
        "async def fetch_series(session: aiohttp.ClientSession, sym: str) -> pd.DataFrame | None:\n",
        "    \"\"\"Fetch TIME_SERIES_DAILY_ADJUSTED for a symbol; return tidy DataFrame or None.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
        "        \"symbol\": sym,\n",
        "        \"outputsize\": \"full\",\n",
        "        \"apikey\": API_KEY,\n",
        "    }\n",
        "    async with RATE_LIMITER:\n",
        "        try:\n",
        "            async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                if resp.status != 200:\n",
        "                    return None\n",
        "                data = await resp.json(content_type=None)\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "            return None\n",
        "\n",
        "    key = next((k for k in data if \"Time Series\" in k), None)\n",
        "    if not key or not data.get(key):\n",
        "        return None\n",
        "\n",
        "    df = (\n",
        "        pd.DataFrame.from_dict(data[key], orient=\"index\")\n",
        "          .reset_index()\n",
        "          .rename(columns={\n",
        "              \"index\": \"date\",\n",
        "              \"1. open\": \"open\",\n",
        "              \"2. high\": \"high\",\n",
        "              \"3. low\": \"low\",\n",
        "              \"4. close\": \"close\",\n",
        "              \"5. adjusted close\": \"adj_close\",\n",
        "              \"6. volume\": \"volume\",\n",
        "              \"7. dividend amount\": \"dividend\",\n",
        "              \"8. split coefficient\": \"split_coef\",\n",
        "          })\n",
        "    )\n",
        "    df[\"symbol\"] = sym\n",
        "    df[\"date\"]   = pd.to_datetime(df[\"date\"]).dt.date\n",
        "    df[\"yyyymm\"] = df[\"date\"].apply(lambda d: d.year * 100 + d.month).astype(\"int64\")\n",
        "\n",
        "    float_cols = [\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"dividend\",\"split_coef\"]\n",
        "    df[float_cols] = df[float_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df[\"volume\"]   = pd.to_numeric(df[\"volume\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    return df\n",
        "\n",
        "async def listing(session: aiohttp.ClientSession, state: str) -> List[str]:\n",
        "    \"\"\"Async, rate-limited Alpha Vantage LISTING_STATUS CSV â†’ list of stock symbols.\"\"\"\n",
        "    params = {\n",
        "        \"function\": \"LISTING_STATUS\",\n",
        "        \"state\": state,           # \"active\" or \"delisted\"\n",
        "        \"datatype\": \"csv\",\n",
        "        \"apikey\": API_KEY,\n",
        "    }\n",
        "    async with RATE_LIMITER:\n",
        "        try:\n",
        "            async with session.get(BASE_URL, params=params, timeout=45) as resp:\n",
        "                if resp.status != 200:\n",
        "                    tqdm.tqdm.write(f\"âš ï¸ listing {state} http {resp.status}\")\n",
        "                    return []\n",
        "                txt = await resp.text()\n",
        "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
        "            tqdm.tqdm.write(f\"âš ï¸ listing {state} fetch error\")\n",
        "            return []\n",
        "    rdr = csv.DictReader(io.StringIO(txt))\n",
        "    return [r[\"symbol\"] for r in rdr if r.get(\"assetType\") == \"Stock\" and r.get(\"symbol\")]\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ BigQuery bootstrapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Dataset\n",
        "try:\n",
        "    bq.get_dataset(f\"{PROJECT}.{DATASET}\")\n",
        "except gex.NotFound:\n",
        "    bq.create_dataset(bigquery.Dataset(f\"{PROJECT}.{DATASET}\"))\n",
        "\n",
        "# Table (partitioned & clustered)\n",
        "table_ref = f\"{PROJECT}.{DATASET}.{TABLE}\"\n",
        "if not any(t.table_id == TABLE for t in bq.list_tables(f\"{PROJECT}.{DATASET}\")):\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"symbol\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"open\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"high\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"low\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"close\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"adj_close\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"volume\", \"INT64\"),\n",
        "        bigquery.SchemaField(\"dividend\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"split_coef\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"yyyymm\", \"INT64\"),\n",
        "    ]\n",
        "    tbl = bigquery.Table(table_ref, schema=schema)\n",
        "    tbl.range_partitioning = bigquery.RangePartitioning(\n",
        "        field=\"yyyymm\",\n",
        "        range_=bigquery.PartitionRange(PART_START, PART_END, 1),\n",
        "    )\n",
        "    tbl.clustering_fields = [\"symbol\"]\n",
        "    bq.create_table(tbl)\n",
        "    tqdm.tqdm.write(f\"âž¡ï¸  created table {table_ref}\")\n",
        "\n",
        "# Ensure we can read schema (for CSV order if needed)\n",
        "tbl = bq.get_table(table_ref)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Load to a single partition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "CSV_COLS = [\n",
        "    \"symbol\", \"date\", \"open\", \"high\", \"low\", \"close\",\n",
        "    \"adj_close\", \"volume\", \"dividend\", \"split_coef\", \"yyyymm\",\n",
        "]\n",
        "\n",
        "def load_partition(df_part: pd.DataFrame, yyyymm: int, retries: int = 3):\n",
        "    \"\"\"Append df_part to TABLE$yyyymm via gzip-CSV (no truncation).\"\"\"\n",
        "    if df_part is None or df_part.empty:\n",
        "        return\n",
        "    df_part = df_part[CSV_COLS]\n",
        "    fd, tmp = tempfile.mkstemp(suffix=\".csv.gz\"); _os.close(fd)\n",
        "    df_part.to_csv(tmp, index=False, compression=\"gzip\", date_format=\"%Y-%m-%d\")\n",
        "\n",
        "    dest = f\"{table_ref}${yyyymm}\"\n",
        "    cfg = bigquery.LoadJobConfig(\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        skip_leading_rows=1,\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "    )\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            with open(tmp, \"rb\") as buf:\n",
        "                job = bq.load_table_from_file(buf, dest, job_config=cfg, rewind=True)\n",
        "            tqdm.tqdm.write(f\"â¬†ï¸  load {dest} rows {len(df_part)} (try {attempt})\")\n",
        "            job.result(timeout=300)\n",
        "            tqdm.tqdm.write(f\"âœ… partition {yyyymm} rows {len(df_part)}\")\n",
        "            break\n",
        "        except (gex.GoogleAPICallError, gex.RetryError, TimeoutError) as e:\n",
        "            tqdm.tqdm.write(f\"âš ï¸  {dest} {e} (try {attempt})\")\n",
        "            time.sleep(10)\n",
        "    _os.remove(tmp)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Checkpoint & helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def read_checkpoint() -> Set[str]:\n",
        "    return set(open(CHECKFILE).read().splitlines()) if _os.path.exists(CHECKFILE) else set()\n",
        "\n",
        "def add_checkpoint(sym: str):\n",
        "    with open(CHECKFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "def add_failure(sym: str):\n",
        "    with open(FAILFILE, \"a\") as f:\n",
        "        f.write(sym + \"\\n\")\n",
        "\n",
        "def symbols_already_loaded() -> Set[str]:\n",
        "    \"\"\"Return distinct symbols already in BigQuery (used for gap-fill).\"\"\"\n",
        "    q = f\"SELECT DISTINCT symbol FROM `{table_ref}`\"\n",
        "    return {row.symbol for row in bq.query(q).result()}\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ De-duplication helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def dedupe_partitions(parts: Set[int]):\n",
        "    \"\"\"De-duplicate rows within each provided yyyymm partition (one row per (symbol, date)).\"\"\"\n",
        "    import tqdm as _tqdm\n",
        "    if not parts:\n",
        "        _tqdm.tqdm.write(\"ðŸ§¹ dedupe: no partitions to process\")\n",
        "        return\n",
        "\n",
        "    sql = f\"\"\"\n",
        "    -- De-duplicate within each requested partition (yyyymm).\n",
        "    FOR part IN (SELECT p FROM UNNEST(@parts) AS p) DO\n",
        "      CREATE OR REPLACE TEMP TABLE tmp AS\n",
        "      SELECT symbol, date, open, high, low, close, adj_close, volume, dividend, split_coef, yyyymm\n",
        "      FROM (\n",
        "        SELECT t.*,\n",
        "               ROW_NUMBER() OVER (PARTITION BY symbol, date ORDER BY yyyymm DESC) AS rn\n",
        "        FROM `{table_ref}` AS t\n",
        "        WHERE yyyymm = part\n",
        "      )\n",
        "      WHERE rn = 1;\n",
        "\n",
        "      DELETE FROM `{table_ref}` WHERE yyyymm = part;\n",
        "\n",
        "      INSERT INTO `{table_ref}` (\n",
        "        symbol, date, open, high, low, close, adj_close, volume, dividend, split_coef, yyyymm\n",
        "      )\n",
        "      SELECT * FROM tmp;\n",
        "    END FOR;\n",
        "    \"\"\"\n",
        "    job = bq.query(\n",
        "        sql,\n",
        "        job_config=bigquery.QueryJobConfig(\n",
        "            query_parameters=[bigquery.ArrayQueryParameter(\"parts\", \"INT64\", sorted(parts))]\n",
        "        ),\n",
        "    )\n",
        "    job.result()\n",
        "    _tqdm.tqdm.write(f\"ðŸ§¹ dedupe: processed {len(parts)} partition(s)\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main async entrypoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "async def main():\n",
        "    # Shared state\n",
        "    sem                         = asyncio.Semaphore(CONCURRENCY)\n",
        "    pending: Dict[int, List[pd.DataFrame]] = defaultdict(list)\n",
        "    pending_rows: Dict[int, int]           = defaultdict(int)\n",
        "    part_locks: Dict[int, asyncio.Lock]    = {}      # per-partition flush locks\n",
        "    touched_parts: Set[int]                 = set()\n",
        "\n",
        "    async def maybe_flush(part: int):\n",
        "        \"\"\"Flush a partition buffer if it exceeds FLUSH_ROWS (with per-partition lock).\"\"\"\n",
        "        lock = part_locks.setdefault(part, asyncio.Lock())\n",
        "        async with lock:\n",
        "            if pending_rows[part] >= FLUSH_ROWS:\n",
        "                batch = pd.concat(pending[part]); pending[part].clear(); pending_rows[part] = 0\n",
        "                await asyncio.to_thread(load_partition, batch, part)\n",
        "\n",
        "    # Single global session (listings + series)\n",
        "    connector = aiohttp.TCPConnector(limit=CONCURRENCY)\n",
        "    async with aiohttp.ClientSession(connector=connector, timeout=aiohttp.ClientTimeout(total=90)) as session:\n",
        "        # Build AV universe\n",
        "        active, delisted = await asyncio.gather(\n",
        "            listing(session, \"active\"),\n",
        "            listing(session, \"delisted\")\n",
        "        )\n",
        "        universe = sorted(set(active + delisted))\n",
        "\n",
        "        # Decide mode\n",
        "        existing = symbols_already_loaded()   # set of symbols in BQ\n",
        "        if len(existing) == 0:\n",
        "            mode = \"full\"\n",
        "            # Full crawl: ignore local checkpoint when building todo to guarantee a truly full refill\n",
        "            todo = universe\n",
        "        else:\n",
        "            mode = \"gapfill\"\n",
        "            # Gap fill: rely on BQ to skip already-loaded symbols; ignore checkpoint here\n",
        "            todo = [s for s in universe if s not in existing]\n",
        "\n",
        "        tqdm.tqdm.write(\n",
        "            f\"Mode: {mode.upper()} | AV universe {len(universe):,} | \"\n",
        "            f\"already in BQ {len(existing):,} | to fetch {len(todo):,}\"\n",
        "        )\n",
        "\n",
        "        # Workers\n",
        "        pbar = tqdm.tqdm(total=len(todo), desc=\"Symbols processed\")\n",
        "        async def worker(sym: str):\n",
        "            async with sem:\n",
        "                df = await fetch_series(session, sym)\n",
        "                if df is not None and not df.empty:\n",
        "                    for part, grp in df.groupby(\"yyyymm\"):\n",
        "                        pending[part].append(grp)\n",
        "                        pending_rows[part] += len(grp)\n",
        "                        touched_parts.add(int(part))\n",
        "                        await maybe_flush(part)\n",
        "                    # Record success; ignored for building gap-fill todo but useful for audits\n",
        "                    add_checkpoint(sym)\n",
        "                else:\n",
        "                    add_failure(sym)\n",
        "                pbar.update(1)\n",
        "\n",
        "        tasks = [asyncio.create_task(worker(sym)) for sym in todo]\n",
        "        await asyncio.gather(*tasks)\n",
        "        pbar.close()\n",
        "\n",
        "    # Final flush for any leftover partitions\n",
        "    for part, dfs in pending.items():\n",
        "        if dfs:\n",
        "            await asyncio.to_thread(load_partition, pd.concat(dfs), part)\n",
        "\n",
        "    # End-of-run de-dup only for touched partitions\n",
        "    await asyncio.to_thread(dedupe_partitions, touched_parts)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    }
  ]
}