{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeIYbr58ASeUd0OQEdy75W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tthandi/magic_formula_project/blob/main/AlphaVantageDataLimitationsTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test of daily adjusted price data for popular tickers\n"
      ],
      "metadata": {
        "id": "0QECCSBxWmzf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejfnwb--WhMs"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 0. Install libraries, import modules, authenticate to GCP\n",
        "# ----------------------------------------------------------------------\n",
        "!pip install --quiet --upgrade pandas google-cloud-storage google-cloud-bigquery\n",
        "\n",
        "import io, json, requests, pandas as pd\n",
        "from google.colab import auth\n",
        "from google.cloud import storage, bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Configuration\n",
        "# ----------------------------------------------------------------------\n",
        "PROJECT_ID  = \"\"\n",
        "DATASET_ID  = \"\"\n",
        "BUCKET_NAME = \"\"\n",
        "LOCATION    = \"\"\n",
        "API_KEY     = \"\"\n",
        "TICKERS     = [\"AAPL\", \"MSFT\", \"IBM\", \"GE\"]\n",
        "YEARS       = [\"2024\", \"2014\", \"2004\", \"1994\", \"1984\"]\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Get or create GCS bucket and BigQuery dataset\n",
        "# ----------------------------------------------------------------------\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "try:\n",
        "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "    print(\"âœ… Reusing bucket:\", BUCKET_NAME)\n",
        "except NotFound:\n",
        "    bucket = storage_client.bucket(BUCKET_NAME)\n",
        "    bucket.storage_class = \"STANDARD\"\n",
        "    storage_client.create_bucket(bucket, location=LOCATION)\n",
        "    print(\"âœ… Created bucket:\", BUCKET_NAME)\n",
        "\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "try:\n",
        "    bq_client.get_dataset(DATASET_ID)\n",
        "    print(\"âœ… BigQuery dataset exists:\", DATASET_ID)\n",
        "except NotFound:\n",
        "    ds = bigquery.Dataset(f\"{PROJECT_ID}.{DATASET_ID}\")\n",
        "    ds.location = LOCATION\n",
        "    bq_client.create_dataset(ds)\n",
        "    print(\"âœ… Created dataset:\", DATASET_ID)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Helper functions\n",
        "# ----------------------------------------------------------------------\n",
        "def clean_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns=lambda c: str(c).lower()\n",
        "                                   .strip().replace(\" \", \"_\").replace(\".\", \"\"))\n",
        "\n",
        "def write_gcs(df: pd.DataFrame, blob_name: str) -> None:\n",
        "    path = f\"/tmp/{blob_name}\"\n",
        "    df.to_csv(path, index=False, encoding=\"utf-8\")\n",
        "    bucket.blob(blob_name).upload_from_filename(path)\n",
        "    print(\"ğŸ“¤\", f\"gs://{BUCKET_NAME}/{blob_name}\")\n",
        "\n",
        "def load_bq(clean_blob: str) -> None:\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{clean_blob[:-4].lower()}\"\n",
        "    cfg = bigquery.LoadJobConfig(\n",
        "        autodetect=True,\n",
        "        skip_leading_rows=1,\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "        write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
        "    )\n",
        "    uri = f\"gs://{BUCKET_NAME}/{clean_blob}\"\n",
        "    bq_client.load_table_from_uri(uri, table_id, job_config=cfg).result()\n",
        "    print(\"âœ… BigQuery:\", table_id)\n",
        "\n",
        "def fetch_listing_status(state=\"active\") -> pd.DataFrame:\n",
        "    r = requests.get(\n",
        "        \"https://www.alphavantage.co/query\",\n",
        "        params={\n",
        "            \"function\": \"LISTING_STATUS\",\n",
        "            \"apikey\"  : API_KEY,\n",
        "            \"state\"   : state,\n",
        "            \"datatype\": \"csv\"\n",
        "        },\n",
        "        timeout=15\n",
        "    )\n",
        "    print(\"ğŸ”\", r.url, \"â†’\", r.status_code, len(r.text), \"bytes\")\n",
        "    r.raise_for_status()\n",
        "    return pd.read_csv(io.StringIO(r.text)) if r.text.strip() else pd.DataFrame()\n",
        "\n",
        "def fetch_json(fn, symbol=None, **extra) -> dict:\n",
        "    base = {\"function\": fn, \"apikey\": API_KEY, **extra}\n",
        "    if symbol:\n",
        "        base[\"symbol\"] = symbol\n",
        "    r = requests.get(\"https://www.alphavantage.co/query\", params=base, timeout=15)\n",
        "    print(\"ğŸ”\", r.url, \"â†’\", r.status_code)\n",
        "    r.raise_for_status()\n",
        "    try:\n",
        "        data = r.json()\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"âš ï¸ non-JSON response\")\n",
        "        return {}\n",
        "    if \"Note\" in data:\n",
        "        print(\"âš ï¸\", data[\"Note\"][:120])\n",
        "    return data\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. LISTING_STATUS (active & delisted) â€“ run once\n",
        "# ----------------------------------------------------------------------\n",
        "for state in (\"active\", \"delisted\"):\n",
        "    df_ls = fetch_listing_status(state)\n",
        "    if not df_ls.empty:\n",
        "        write_gcs(df_ls,                 f\"listing_status_{state}_raw.csv\")\n",
        "        cleaned = clean_cols(df_ls)\n",
        "        clean_name = f\"listing_status_{state}.csv\"\n",
        "        write_gcs(cleaned, clean_name)\n",
        "        load_bq(clean_name)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. Ticker Ã— Year loop for daily-adjusted data\n",
        "# ----------------------------------------------------------------------\n",
        "for tic in TICKERS:\n",
        "    print(f\"\\nğŸ“ˆ {tic} â€” daily-adjusted history slices\")\n",
        "    full_json = fetch_json(\"TIME_SERIES_DAILY_ADJUSTED\", tic, outputsize=\"full\")\n",
        "    all_daily = full_json.get(\"Time Series (Daily)\", {})\n",
        "\n",
        "    for yr in YEARS:\n",
        "        rows = {d: v for d, v in all_daily.items() if d.startswith(yr)}\n",
        "        if not rows:\n",
        "            print(f\"â„¹ï¸ {tic} has no data for {yr} â€” skipped\")\n",
        "            continue\n",
        "\n",
        "        df = (\n",
        "            pd.DataFrame.from_dict(rows, orient=\"index\")\n",
        "              .reset_index()\n",
        "              .rename(columns={\"index\": \"date\"})\n",
        "        )\n",
        "\n",
        "        raw_name   = f\"{tic}_daily_adjusted_{yr}_raw.csv\"\n",
        "        clean_name = f\"{tic}_daily_adjusted_{yr}.csv\"\n",
        "        write_gcs(df, raw_name)\n",
        "        write_gcs(clean_cols(df), clean_name)\n",
        "        load_bq(clean_name)\n",
        "\n",
        "print(\"\\n Finished â€” raw CSVs in gs://{} and cleaned tables in BigQuery.\".format(BUCKET_NAME))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for adjusted price data for popular and delisted tickers\n"
      ],
      "metadata": {
        "id": "3xxaZwbYWwTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "av_history_depth_plus.py  (revised â€“ no CIK assumption)\n",
        "\"\"\"\n",
        "\n",
        "import os, io, csv, json, time, requests, pandas as pd\n",
        "from datetime import date\n",
        "\n",
        "API_KEY = os.getenv(\"AV_API_KEY\")\n",
        "if not API_KEY:\n",
        "    raise SystemExit(\"export AV_API_KEY first\")\n",
        "\n",
        "BASE  = \"https://www.alphavantage.co/query\"\n",
        "SLEEP = 0.8    # throttle ~75 req/min\n",
        "\n",
        "# â”€â”€ ticker universe (active + delisted sample) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "LONG_LIVED = {\n",
        "    \"IBM\": \"International Business Machines\",\n",
        "    \"GE\":  \"General Electric\",\n",
        "    \"KO\":  \"Coca-Cola Company\",\n",
        "    \"PG\":  \"Procter & Gamble\",\n",
        "}\n",
        "DELISTED = {\n",
        "    \"USG\":   \"USG Corp â€“ private 2019\",\n",
        "    \"DWA\":   \"DreamWorks â€“ bought 2016\",\n",
        "    \"SKUL\":  \"Skullcandy â€“ private 2016\",\n",
        "    \"SYX\":   \"Systemax â€“ ticker now GIC\",\n",
        "    \"KRFT\":  \"Kraft Foods Group â€“ merged 2015\",\n",
        "    \"LEHMQ\": \"Lehman Brothers â€“ bankruptcy 2008\",\n",
        "    \"ENRNQ\": \"Enron â€“ bankruptcy 2001\",\n",
        "}\n",
        "TICKERS = {**LONG_LIVED, **DELISTED}\n",
        "\n",
        "ENDPOINTS = {\n",
        "    \"DAILY\":   {\"function\": \"TIME_SERIES_DAILY_ADJUSTED\",   \"outputsize\": \"full\"},\n",
        "    \"WEEKLY\":  {\"function\": \"TIME_SERIES_WEEKLY_ADJUSTED\"},\n",
        "    \"MONTHLY\": {\"function\": \"TIME_SERIES_MONTHLY_ADJUSTED\"},\n",
        "}\n",
        "\n",
        "# â”€â”€ download listing snapshots once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def fetch_listing(state: str) -> pd.DataFrame:\n",
        "    r = requests.get(BASE, params={\n",
        "        \"function\": \"LISTING_STATUS\",\n",
        "        \"state\": state,\n",
        "        \"datatype\": \"csv\",\n",
        "        \"apikey\": API_KEY}, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    df = pd.read_csv(io.StringIO(r.text))\n",
        "    df.columns = [c.lower() for c in df.columns]      # normalise\n",
        "    return df\n",
        "\n",
        "print(\"Downloading LISTING_STATUS snapshots â€¦\")\n",
        "active_df   = fetch_listing(\"active\")\n",
        "delisted_df = fetch_listing(\"delisted\")\n",
        "active_set   = set(active_df[\"symbol\"])\n",
        "delisted_set = set(delisted_df[\"symbol\"])\n",
        "\n",
        "# (optional) build successor map here if you have manual aliases\n",
        "SUCCESSOR_MAP = {\n",
        "    # \"SYX\": \"GIC\",   # example\n",
        "}\n",
        "\n",
        "# â”€â”€ simple resolver (no CIK fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def resolve_symbol(sym: str) -> str:\n",
        "    sym_clean = sym.rstrip(\"QPK\")           # drop OTC suffixes\n",
        "    return SUCCESSOR_MAP.get(sym_clean, sym_clean)\n",
        "\n",
        "# â”€â”€ fetch one series helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def fetch_series(sym: str, params: dict) -> tuple[pd.DataFrame, str]:\n",
        "    r = requests.get(BASE, params=params | {\"symbol\": sym, \"apikey\": API_KEY})\n",
        "    if r.status_code != 200:\n",
        "        return pd.DataFrame(), f\"http {r.status_code}\"\n",
        "    try:\n",
        "        j = r.json()\n",
        "    except json.JSONDecodeError:\n",
        "        return pd.DataFrame(), \"non-json\"\n",
        "    if \"Note\" in j:           return pd.DataFrame(), \"rate-limit\"\n",
        "    if \"Error Message\" in j:  return pd.DataFrame(), \"api-error\"\n",
        "    key = next((k for k in j if \"Series\" in k), None)\n",
        "    if not (key and j[key]):  return pd.DataFrame(), \"empty\"\n",
        "    df = (pd.DataFrame.from_dict(j[key], orient=\"index\")\n",
        "            .reset_index().rename(columns={\"index\":\"date\"}))\n",
        "    return df, \"ok\"\n",
        "\n",
        "# â”€â”€ main probe loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "records = []\n",
        "for orig, note in TICKERS.items():\n",
        "    sym = resolve_symbol(orig)\n",
        "    rec = {\"symbol\": orig, \"resolved\": sym, \"note\": note,\n",
        "           \"in_active\": sym in active_set, \"in_delisted\": sym in delisted_set}\n",
        "    earliest = latest = rows = None\n",
        "\n",
        "    for tag, ep in ENDPOINTS.items():\n",
        "        df, reason = fetch_series(sym, ep)\n",
        "        time.sleep(SLEEP)\n",
        "        rec[f\"{tag.lower()}_status\"] = \"âœ…\" if not df.empty else f\"âŒ ({reason})\"\n",
        "\n",
        "        if earliest is None and not df.empty:          # first successful series\n",
        "            earliest = df[\"date\"].min()\n",
        "            latest   = df[\"date\"].max()\n",
        "            rows     = len(df)\n",
        "            df.to_csv(f\"{orig}_{tag.lower()}_alpha.csv\", index=False)\n",
        "\n",
        "    rec.update({\"earliest\": earliest or \"n/a\",\n",
        "                \"latest\"  : latest   or \"n/a\",\n",
        "                \"rows\"    : rows     or 0})\n",
        "    records.append(rec)\n",
        "\n",
        "# â”€â”€ summary table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cols = [\"symbol\",\"resolved\",\"note\",\"earliest\",\"latest\",\"rows\",\n",
        "        \"daily_status\",\"weekly_status\",\"monthly_status\",\"in_delisted\",\"in_active\"]\n",
        "df_out = pd.DataFrame(records)[cols]\n",
        "\n",
        "print(f\"\\nAlpha Vantage history depth â€” {date.today()}\\n\")\n",
        "print(df_out.to_markdown(index=False))\n"
      ],
      "metadata": {
        "id": "pfEoJrOnWjum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}